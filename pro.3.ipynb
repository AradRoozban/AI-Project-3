{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a0cb9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diffusers not available: No module named 'diffusers'\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gradio'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 77\u001b[0m\n\u001b[0;32m     74\u001b[0m     WHISPER_AVAILABLE \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# Gradio\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgr\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# Config\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[0;32m     82\u001b[0m EMOTION_MODEL_ID \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mj-hartmann/emotion-english-distilroberta-base\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# good zero-shot model\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gradio'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Emotion Painter - Advanced Gradio App\n",
    "Single-file prototype: app.py\n",
    "\n",
    "Features:\n",
    "- Text and audio input (transcription via whisper or faster-whisper)\n",
    "- Emotion detection (transformers classifier)\n",
    "- Emotion-to-prompt mapping with blending by confidence\n",
    "- Image generation with Stable Diffusion (diffusers) with CPU/GPU fallback\n",
    "- Basic safety checker (CLIP-based NSFW filter from diffusers)\n",
    "- Gradio UI with preview, prompt display, download\n",
    "- Configurable generation settings, deterministic seeds, caching of prompts\n",
    "\n",
    "Notes:\n",
    "- This is a production-minded prototype. For production you'll want\n",
    "  to add rate-limiting, persistent storage (S3/DB), authentication, and a job queue\n",
    "  for high-resolution renders.\n",
    "\n",
    "Requirements (put in requirements.txt):\n",
    "# core\n",
    "transformers>=4.30.0\n",
    "torch>=2.0.0\n",
    "gradio>=3.30.0\n",
    "diffusers>=0.19.0\n",
    "accelerate\n",
    "safetensors\n",
    "ftfy\n",
    "intel-openmp==2023.1.0  # if using CPU-only on some platforms\n",
    "\n",
    "# optional (whisper)\n",
    "openai-whisper>=20230314   # or faster-whisper for speed\n",
    "# faster-whisper recommended on CPU for speed\n",
    "# faster-whisper\n",
    "# git+https://github.com/guillaumekln/faster-whisper\n",
    "\n",
    "# image processing\n",
    "Pillow\n",
    "numpy\n",
    "\n",
    "# (if you plan to run on GPU) - install matching torch / cuda\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# app.py\n",
    "import os\n",
    "import io\n",
    "import math\n",
    "import time\n",
    "import json\n",
    "import hashlib\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "# diffusers imports\n",
    "try:\n",
    "    from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\n",
    "    from diffusers.pipelines.stable_diffusion.safety_checker import StableDiffusionSafetyChecker\n",
    "    from transformers import CLIPFeatureExtractor\n",
    "    DIFFUSERS_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    print(\"diffusers not available:\", e)\n",
    "    DIFFUSERS_AVAILABLE = False\n",
    "\n",
    "# whisper (transcription)\n",
    "try:\n",
    "    import whisper\n",
    "    WHISPER_AVAILABLE = True\n",
    "except Exception:\n",
    "    WHISPER_AVAILABLE = False\n",
    "\n",
    "# Gradio\n",
    "import gradio as gr\n",
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "EMOTION_MODEL_ID = \"j-hartmann/emotion-english-distilroberta-base\"  # good zero-shot model\n",
    "SD_MODEL_ID = \"runwayml/stable-diffusion-v1-5\"  # change to your preferred checkpoint\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "CACHE_DIR = os.getenv(\"EMOTION_PAINTER_CACHE\", \".ep_cache\")\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Utilities\n",
    "# -----------------------------\n",
    "\n",
    "def deterministic_hash(s: str) -> int:\n",
    "    h = hashlib.sha256(s.encode(\"utf-8\")).hexdigest()\n",
    "    return int(h[:16], 16)\n",
    "\n",
    "\n",
    "def to_pil(image_array: np.ndarray) -> Image.Image:\n",
    "    if image_array.dtype != np.uint8:\n",
    "        image_array = (255 * np.clip(image_array, 0, 1)).astype(np.uint8)\n",
    "    return Image.fromarray(image_array)\n",
    "\n",
    "# -----------------------------\n",
    "# Emotion classifier wrapper\n",
    "# -----------------------------\n",
    "\n",
    "print(\"Loading emotion classifier...\", flush=True)\n",
    "try:\n",
    "    emotion_pipe = pipeline(\"text-classification\", model=EMOTION_MODEL_ID, return_all_scores=True, device=0 if DEVICE==\"cuda\" else -1)\n",
    "except Exception as e:\n",
    "    print(\"Failed to load emotion model; will attempt CPU fallback.\", e)\n",
    "    emotion_pipe = pipeline(\"text-classification\", model=EMOTION_MODEL_ID, return_all_scores=True, device=-1)\n",
    "\n",
    "# Normalized emotion labels we will support (map model outputs to our canonical set)\n",
    "CANONICAL_EMOTIONS = [\"joy\", \"sadness\", \"anger\", \"fear\", \"calmness\"]\n",
    "# map a broader set of labels (GoEmotions-like) into canonical categories\n",
    "LABEL_MAP = {\n",
    "    # joy-like\n",
    "    \"joy\": \"joy\", \"happiness\": \"joy\", \"amusement\": \"joy\", \"excitement\": \"joy\", \"optimism\": \"joy\",\n",
    "    # sadness-like\n",
    "    \"sadness\": \"sadness\", \"grief\": \"sadness\", \"disappointment\": \"sadness\",\n",
    "    # anger-like\n",
    "    \"anger\": \"anger\", \"annoyance\": \"anger\", \"frustration\": \"anger\",\n",
    "    # fear-like\n",
    "    \"fear\": \"fear\", \"anxiety\": \"fear\", \"nervousness\": \"fear\",\n",
    "    # calm-like / neutral\n",
    "    \"neutral\": \"calmness\", \"calmness\": \"calmness\", \"relief\": \"calmness\", \"contentment\": \"calmness\"\n",
    "}\n",
    "\n",
    "\n",
    "def normalize_label(label: str) -> str:\n",
    "    lab = label.lower()\n",
    "    return LABEL_MAP.get(lab, lab if lab in CANONICAL_EMOTIONS else \"calmness\")\n",
    "\n",
    "\n",
    "def predict_emotions(text: str, top_k: int = 5) -> List[Tuple[str, float]]:\n",
    "    \"\"\"Return a sorted list of (emotion, score) mapped to canonical emotions\n",
    "    \"\"\"\n",
    "    if not text or not text.strip():\n",
    "        return [(\"calmness\", 1.0)]\n",
    "    try:\n",
    "        raw = emotion_pipe(text)[0]\n",
    "    except Exception as e:\n",
    "        # fallback: very simple heuristics\n",
    "        txt = text.lower()\n",
    "        scoremap = {\"joy\":0.0,\"sadness\":0.0,\"anger\":0.0,\"fear\":0.0,\"calmness\":0.0}\n",
    "        if any(w in txt for w in [\"happy\",\"joy\",\"glad\",\"excited\",\"love\",\"yay\"]): scoremap['joy'] += 0.9\n",
    "        if any(w in txt for w in [\"sad\",\"down\",\"lonely\",\"depressed\"]): scoremap['sadness'] += 0.9\n",
    "        if any(w in txt for w in [\"angry\",\"mad\",\"furious\",\"hate\"]): scoremap['anger'] += 0.9\n",
    "        if any(w in txt for w in [\"scared\",\"afraid\",\"nervous\",\"anxious\"]): scoremap['fear'] += 0.9\n",
    "        if all(v==0 for v in scoremap.values()): scoremap['calmness'] = 1.0\n",
    "        return sorted(scoremap.items(), key=lambda x:-x[1])\n",
    "\n",
    "    # map labels\n",
    "    agg = {}\n",
    "    for entry in raw:\n",
    "        lbl = normalize_label(entry['label'])\n",
    "        agg[lbl] = agg.get(lbl, 0.0) + entry['score']\n",
    "    # normalize\n",
    "    total = sum(agg.values()) or 1.0\n",
    "    items = [(k, v/total) for k, v in agg.items()]\n",
    "    items_sorted = sorted(items, key=lambda x: -x[1])[:top_k]\n",
    "    # ensure canonical coverage\n",
    "    present = {k for k,_ in items_sorted}\n",
    "    for emo in CANONICAL_EMOTIONS:\n",
    "        if emo not in present:\n",
    "            items_sorted.append((emo, 0.0))\n",
    "    return items_sorted\n",
    "\n",
    "# -----------------------------\n",
    "# Emotion -> prompt mapping\n",
    "# -----------------------------\n",
    "EMOTION_MAP = {\n",
    "    \"joy\": {\"colors\":\"bright gold, warm orange\", \"attrs\":\"sunlit open field, floating ribbons, soft bokeh lights\", \"style\":\"impressionist watercolor\"},\n",
    "    \"sadness\": {\"colors\":\"deep blue, slate grey\", \"attrs\":\"gentle rain, empty bench, reflected puddles\", \"style\":\"soft pastel realism\"},\n",
    "    \"anger\": {\"colors\":\"crimson, ebony black\", \"attrs\":\"stormy sky, jagged shards, splintered geometry\", \"style\":\"expressionist abstract\"},\n",
    "    \"fear\": {\"colors\":\"desaturated green, cold blue\", \"attrs\":\"fog, long corridor, shadowed corners\", \"style\":\"cinematic chiaroscuro\"},\n",
    "    \"calmness\": {\"colors\":\"mint, pale pink, soft cream\", \"attrs\":\"still water, smooth gradient, gentle horizon\", \"style\":\"minimal japanese ink\"}\n",
    "}\n",
    "\n",
    "PROMPT_TEMPLATE = \"{emotion} mood painting, {attrs}, colors: {colors}, composition: {composition}, style: {style}, ultra-detailed, high resolution, artstation, 8k\"\n",
    "\n",
    "\n",
    "def blend_prompt(emotions: List[Tuple[str, float]], max_terms: int=3) -> str:\n",
    "    # take top N emotions and blend attributes by weight\n",
    "    top = [(e,w) for e,w in emotions if w>0]\n",
    "    top = sorted(top, key=lambda x:-x[1])[:max_terms]\n",
    "    if not top:\n",
    "        top = [(\"calmness\", 1.0)]\n",
    "    # normalized weights\n",
    "    s = sum(w for _,w in top) or 1.0\n",
    "    parts = []\n",
    "    colors = []\n",
    "    styles = []\n",
    "    attrs = []\n",
    "    for e,w in top:\n",
    "        m = EMOTION_MAP.get(e, EMOTION_MAP['calmness'])\n",
    "        ratio = w/s\n",
    "        # include textual hints spread by weight\n",
    "        attrs.append(f\"({m['attrs']}) x{ratio:.2f}\")\n",
    "        colors.append(m['colors'])\n",
    "        styles.append(m['style'])\n",
    "    # simple composition heuristic\n",
    "    composition = \"wide and airy\" if top[0][0] in [\"joy\",\"calmness\"] else \"tight and dramatic\"\n",
    "    emotion_names = \", \".join([f\"{e}({w:.2f})\" for e,w in top])\n",
    "    prompt = PROMPT_TEMPLATE.format(\n",
    "        emotion=emotion_names,\n",
    "        attrs='; '.join(attrs),\n",
    "        colors=' / '.join(colors),\n",
    "        composition=composition,\n",
    "        style=' + '.join(styles)\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "# -----------------------------\n",
    "# Stable Diffusion generator wrapper\n",
    "# -----------------------------\n",
    "pipe = None\n",
    "safety_model = None\n",
    "feature_extractor = None\n",
    "\n",
    "if DIFFUSERS_AVAILABLE:\n",
    "    try:\n",
    "        print(f\"Loading Stable Diffusion model ({SD_MODEL_ID}) to {DEVICE}...\")\n",
    "        pipe = StableDiffusionPipeline.from_pretrained(SD_MODEL_ID, torch_dtype=torch.float16 if DEVICE==\"cuda\" else torch.float32)\n",
    "        # use a faster scheduler\n",
    "        pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "        if DEVICE == \"cuda\":\n",
    "            pipe = pipe.to(\"cuda\")\n",
    "        else:\n",
    "            pipe = pipe.to(\"cpu\")\n",
    "        # safety checker\n",
    "        try:\n",
    "            safety_model = StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\")\n",
    "            feature_extractor = CLIPFeatureExtractor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        except Exception as e:\n",
    "            print(\"Safety checker not available:\", e)\n",
    "    except Exception as e:\n",
    "        print(\"Could not initialize Stable Diffusion pipeline:\", e)\n",
    "        pipe = None\n",
    "\n",
    "\n",
    "def check_safety(image: Image.Image) -> Tuple[bool, float]:\n",
    "    \"\"\"Return (is_safe, unsafe_score) - simple wrapper. If no safety model, return safe True.\"\"\"\n",
    "    if safety_model is None or feature_extractor is None:\n",
    "        return True, 0.0\n",
    "    # convert\n",
    "    image_np = np.array(image.convert(\"RGB\"))\n",
    "    inputs = feature_extractor(images=[image_np], return_tensors=\"pt\")\n",
    "    try:\n",
    "        safety_output = safety_model(images=image_np, clip_input=inputs.pixel_values)\n",
    "        has_nsfw_concept = safety_output.nsfw_content_detected[0]\n",
    "        score = float(max(safety_output.nsfw_score[0].tolist())) if hasattr(safety_output, 'nsfw_score') else (1.0 if has_nsfw_concept else 0.0)\n",
    "        return (not has_nsfw_concept), score\n",
    "    except Exception as e:\n",
    "        print(\"Safety check failed:\", e)\n",
    "        return True, 0.0\n",
    "\n",
    "\n",
    "def generate_from_prompt(prompt: str, seed: Optional[int]=None, guidance_scale: float=7.5, steps: int=30, height: int=512, width: int=512):\n",
    "    if pipe is None:\n",
    "        raise RuntimeError(\"Stable Diffusion pipeline not initialized. Install diffusers and the model weights.\")\n",
    "    generator = None\n",
    "    if seed is None:\n",
    "        seed = deterministic_hash(prompt) & 0xFFFFFFFF\n",
    "    if DEVICE == \"cuda\":\n",
    "        generator = torch.Generator(device=\"cuda\").manual_seed(seed)\n",
    "    else:\n",
    "        generator = torch.Generator(device=\"cpu\").manual_seed(seed)\n",
    "\n",
    "    with torch.autocast(\"cuda\") if DEVICE==\"cuda\" else torch.cpu.amp.autocast(enabled=False):\n",
    "        image = pipe(prompt, guidance_scale=guidance_scale, num_inference_steps=steps, generator=generator, height=height, width=width).images[0]\n",
    "\n",
    "    is_safe, score = check_safety(image)\n",
    "    return image, is_safe, score\n",
    "\n",
    "# -----------------------------\n",
    "# Transcription (Whisper) helper\n",
    "# -----------------------------\n",
    "\n",
    "def transcribe_audio_file(filepath: str) -> str:\n",
    "    if not WHISPER_AVAILABLE:\n",
    "        # fallback: raise informative error\n",
    "        raise RuntimeError(\"Whisper is not installed. Install 'whisper' or 'faster-whisper' for transcription.\")\n",
    "    model = whisper.load_model(\"small\")  # choose model size for quality/speed tradeoff\n",
    "    result = model.transcribe(filepath)\n",
    "    return result.get('text', '')\n",
    "\n",
    "# -----------------------------\n",
    "# Caching utility\n",
    "# -----------------------------\n",
    "\n",
    "def cache_image_for_prompt(prompt: str, image: Image.Image) -> str:\n",
    "    key = hashlib.sha256(prompt.encode('utf-8')).hexdigest()\n",
    "    path = os.path.join(CACHE_DIR, f\"{key}.png\")\n",
    "    image.save(path)\n",
    "    return path\n",
    "\n",
    "def load_cached_image(prompt: str) -> Optional[str]:\n",
    "    key = hashlib.sha256(prompt.encode('utf-8')).hexdigest()\n",
    "    path = os.path.join(CACHE_DIR, f\"{key}.png\")\n",
    "    if os.path.exists(path):\n",
    "        return path\n",
    "    return None\n",
    "\n",
    "# -----------------------------\n",
    "# Gradio app glue\n",
    "# -----------------------------\n",
    "\n",
    "def pipeline_handler(text_input: str, audio_file, seed: Optional[int], guidance_scale: float, steps: int, width: int, height: int, preview_low_res: bool):\n",
    "    # 1. transcribe if audio\n",
    "    if audio_file is not None:\n",
    "        audio_path = audio_file.name\n",
    "        try:\n",
    "            text = transcribe_audio_file(audio_path)\n",
    "        except Exception as e:\n",
    "            return None, f\"Transcription error: {e}\", \"\", []\n",
    "    else:\n",
    "        text = text_input or \"\"\n",
    "\n",
    "    # 2. emotion detection\n",
    "    emotions = predict_emotions(text)\n",
    "    top_emotions_display = [(e, float(s)) for e,s in emotions]\n",
    "\n",
    "    # 3. prompt generation\n",
    "    prompt = blend_prompt(emotions)\n",
    "\n",
    "    # 4. check cache\n",
    "    cached = load_cached_image(f\"{prompt}|{seed}|{guidance_scale}|{steps}|{width}x{height}\")\n",
    "    if cached:\n",
    "        img = Image.open(cached).convert(\"RGB\")\n",
    "        return img, prompt, json.dumps(top_emotions_display), top_emotions_display\n",
    "\n",
    "    # 5. generate (optionally generate low-res preview first)\n",
    "    try:\n",
    "        if preview_low_res:\n",
    "            # quick low-res preview for UX\n",
    "            preview_prompt = prompt + \", low resolution preview, fast\"\n",
    "            preview_img, safe, score = generate_from_prompt(preview_prompt, seed=seed, guidance_scale=guidance_scale, steps=max(10, steps//3), height=256, width=256)\n",
    "            # upscale or keep as preview\n",
    "        img, safe, score = generate_from_prompt(prompt, seed=seed, guidance_scale=guidance_scale, steps=steps, height=height, width=width)\n",
    "    except Exception as e:\n",
    "        return None, f\"Generation error: {e}\", prompt, top_emotions_display\n",
    "\n",
    "    # 6. safety\n",
    "    if not safe:\n",
    "        # mask or refuse\n",
    "        return None, f\"Generated image flagged as unsafe (score {score:.3f}). Try a different input.\", prompt, top_emotions_display\n",
    "\n",
    "    # 7. cache and return\n",
    "    cache_image_for_prompt(f\"{prompt}|{seed}|{guidance_scale}|{steps}|{width}x{height}\", img)\n",
    "\n",
    "    return img, prompt, json.dumps(top_emotions_display), top_emotions_display\n",
    "\n",
    "# Build Gradio interface\n",
    "with gr.Blocks(title=\"Emotion Painter\", css=\".gradio-container {background: linear-gradient(180deg, #f6fbff, #fff);} .card {border-radius: 12px}\") as demo:\n",
    "    gr.Markdown(\"# ðŸŽ¨ Emotion Painter â€” Advanced prototype\")\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=2):\n",
    "            txt = gr.Textbox(label=\"Write your feeling (or leave blank to upload audio)\", lines=3, placeholder=\"e.g. I just aced my exam and I feel on top of the world!\")\n",
    "            aud = gr.Audio(source=\"upload\", type=\"filepath\", label=\"Or upload an audio recording (wav/m4a)\")\n",
    "            with gr.Row():\n",
    "                seed_in = gr.Number(value=None, label=\"Seed (optional)\")\n",
    "                guidance = gr.Slider(minimum=1.0, maximum=12.0, step=0.1, value=7.5, label=\"Guidance Scale\")\n",
    "            with gr.Row():\n",
    "                steps_in = gr.Slider(minimum=10, maximum=50, step=1, value=30, label=\"Steps\")\n",
    "                low_res = gr.Checkbox(value=True, label=\"Generate fast low-res preview first\")\n",
    "            with gr.Row():\n",
    "                width_in = gr.Dropdown(choices=[256, 512, 768], value=512, label=\"Width\")\n",
    "                height_in = gr.Dropdown(choices=[256, 512, 768], value=512, label=\"Height\")\n",
    "            gen_btn = gr.Button(\"Create Artwork\")\n",
    "            note = gr.Markdown(\"Tip: Use short expressive sentences. For audio, try to speak clearly. Model quality depends on installed weights and GPU availability.\")\n",
    "        with gr.Column(scale=3):\n",
    "            output_image = gr.Image(label=\"Generated artwork\", interactive=False)\n",
    "            output_prompt = gr.Textbox(label=\"Prompt used for generation\", lines=2)\n",
    "            detected = gr.JSON(label=\"Detected emotions (top)\"\n",
    "                               )\n",
    "    gen_btn.click(fn=pipeline_handler, inputs=[txt, aud, seed_in, guidance, steps_in, width_in, height_in, low_res], outputs=[output_image, output_prompt, detected, gr.State()])\n",
    "\n",
    "# Launch helper\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"Starting demo on {DEVICE}. Diffusers available: {DIFFUSERS_AVAILABLE}. Whisper available: {WHISPER_AVAILABLE}\")\n",
    "    demo.launch(server_name=\"0.0.0.0\", server_port=int(os.getenv(\"PORT\", 7860)), share=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27934254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting gradio-client\n",
      "  Using cached gradio_client-1.11.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: fsspec in c:\\users\\eye net\\appdata\\roaming\\python\\python310\\site-packages (from gradio-client) (2025.3.2)\n",
      "Requirement already satisfied: httpx>=0.24.1 in c:\\users\\eye net\\appdata\\roaming\\python\\python310\\site-packages (from gradio-client) (0.28.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.3 in c:\\users\\eye net\\appdata\\roaming\\python\\python310\\site-packages (from gradio-client) (0.34.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\eye net\\appdata\\roaming\\python\\python310\\site-packages (from gradio-client) (25.0)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in c:\\users\\eye net\\appdata\\roaming\\python\\python310\\site-packages (from gradio-client) (4.13.2)\n",
      "Requirement already satisfied: websockets<16.0,>=10.0 in c:\\users\\eye net\\appdata\\roaming\\python\\python310\\site-packages (from gradio-client) (15.0.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\eye net\\appdata\\roaming\\python\\python310\\site-packages (from httpx>=0.24.1->gradio-client) (4.9.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\eye net\\appdata\\roaming\\python\\python310\\site-packages (from httpx>=0.24.1->gradio-client) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\eye net\\appdata\\roaming\\python\\python310\\site-packages (from httpx>=0.24.1->gradio-client) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\eye net\\appdata\\roaming\\python\\python310\\site-packages (from httpx>=0.24.1->gradio-client) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\eye net\\appdata\\roaming\\python\\python310\\site-packages (from httpcore==1.*->httpx>=0.24.1->gradio-client) (0.16.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\eye net\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub>=0.19.3->gradio-client) (3.18.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\eye net\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub>=0.19.3->gradio-client) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\eye net\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub>=0.19.3->gradio-client) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\eye net\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub>=0.19.3->gradio-client) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\eye net\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.19.3->gradio-client) (0.4.6)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\eye net\\appdata\\roaming\\python\\python310\\site-packages (from anyio->httpx>=0.24.1->gradio-client) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\eye net\\appdata\\roaming\\python\\python310\\site-packages (from anyio->httpx>=0.24.1->gradio-client) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\eye net\\appdata\\roaming\\python\\python310\\site-packages (from requests->huggingface-hub>=0.19.3->gradio-client) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\eye net\\appdata\\roaming\\python\\python310\\site-packages (from requests->huggingface-hub>=0.19.3->gradio-client) (2.4.0)\n",
      "Using cached gradio_client-1.11.1-py3-none-any.whl (324 kB)\n",
      "Installing collected packages: gradio-client\n",
      "Successfully installed gradio-client-1.11.1\n"
     ]
    }
   ],
   "source": [
    "! pip install gradio-client"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
